{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines (RBMs)\n",
    "Let us assume we have some random samples of fixed-length binary sequences that we wish to represent in an RBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1,1,1,0,0,0],[1,0,1,0,0,0],[1,1,1,0,0,0],[0,0,1,1,1,0], [0,0,1,1,0,0],[0,0,1,1,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the model\n",
    "\n",
    "The RBM is an energy-based model defined by a quadratic weight matrix $W$ and linear biases $b$ and $c$.  The energy $E(v, h)$ is a function of the binary visible units $v$ and the hidden units $h$.\n",
    "\n",
    "$-E(v, h)=v^T W h + c^T h + b^T v$\n",
    "\n",
    "Our RBM has 6 visible, and say 2 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_visible = 6 # = data.shape[0]\n",
    "num_hidden  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.random.randn(num_visible, num_hidden)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append the biases to the weight matrix, and later add respective terms to $h$ and $v$.\n",
    "\n",
    "First, we append $c^T h$, which addes 2 parameters to $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.insert(weights, 0, 0, axis=0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add $b^T v$, adding 6 more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.insert(weights, 0, 0, axis=1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "learning_rate = 0.1\n",
    "num_examples = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training data is given as a row in `data`.  We append an extra column for the bias term $b^T v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = np.insert(data, 0, 1, axis=1)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need to compute the hidden activations, probabilities, and states.\n",
    "\n",
    "- Activation: $a(v)=v^T W + b$\n",
    "- Probability: $p(h=1|v)=1/(1+\\exp{(-a(v))})$\n",
    "- State: $h = \\langle h \\rangle$.  The state is obtained by Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def compute_hidden(visible):\n",
    "    num_examples = visible.shape[0]\n",
    "    pos_hidden_activations = np.dot(visible, weights)      \n",
    "    pos_hidden_probs = logistic(pos_hidden_activations)\n",
    "    pos_hidden_states = pos_hidden_probs > np.random.rand(num_examples, num_hidden + 1)\n",
    "    return pos_hidden_activations, pos_hidden_probs, pos_hidden_states\n",
    "\n",
    "test_data = training_data[:1]\n",
    "a, p, hidden_states = compute_hidden(test_data)\n",
    "print(\"input data:\", test_data)\n",
    "print(\"activation:\", a)\n",
    "print(\"probabilit:\", p)\n",
    "print(\"hidden sta:\", hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to compute the visible units to compare with the visible activations, probability.\n",
    "\n",
    "- visible activation: $a(h) = W h + c^T h$\n",
    "- visible probability: $P(a=1|h) = 1/(1+\\exp{(-a(h))})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_visible(hidden_states):\n",
    "    num_examples = hidden_states.shape[0]\n",
    "    neg_visible_activations = np.dot(hidden_states, weights.T)\n",
    "    neg_visible_probs = logistic(neg_visible_activations)\n",
    "    neg_visible_probs[:, 0] = 1 # Fix the bias unit.\n",
    "    return neg_visible_probs\n",
    "\n",
    "neg_visible_probs = compute_visible(hidden_states)\n",
    "print(\"negative visible probabilities:\", neg_visible_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing probabilities for the visible units, we can use these to estimate hidden units again, a process called daydreaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daydream(neg_visible_probs):\n",
    "    neg_hidden_activations = np.dot(neg_visible_probs, weights)\n",
    "    neg_hidden_probs = logistic(neg_hidden_activations)\n",
    "    return neg_hidden_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done in three steps:\n",
    "\n",
    "- Estimate the hidden states by Gibbs sampling.\n",
    "- From step 1 compute the probability of the visible units.  Comparison with the training data gives us the error function.\n",
    "- Update the weights by minimizing the Energy.\n",
    "\n",
    "Let's take the derivative of $E(v, h)$:\n",
    "\n",
    "$\\partial_{W_{ij}} E = -v^T h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "for epoch in range(nb_epoch):\n",
    "    pos_hidden_act, pos_hidden_prob, pos_hidden_stat = compute_hidden(training_data)\n",
    "    \n",
    "    neg_visible_prob = compute_visible(pos_hidden_stat)\n",
    "    neg_hidden_prob = daydream(neg_visible_prob)\n",
    "    \n",
    "    pos_associations = np.dot(training_data.T, pos_hidden_prob)\n",
    "    neg_associations = np.dot(neg_visible_prob.T, neg_hidden_prob)\n",
    "    \n",
    "    # Update the weights:\n",
    "    weights += learning_rate * ((pos_associations-neg_associations)/float(num_examples))\n",
    "    \n",
    "    # Error:\n",
    "    err = np.sum( (training_data - neg_visible_prob)**2 )\n",
    "    error.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(error)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
